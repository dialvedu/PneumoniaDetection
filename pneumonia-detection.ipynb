{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dialvedu/pneumonia-detection?scriptVersionId=99474288\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Pneumonia Detection in Chest X-Ray Images\n\nBy: Diego Vergara BSc","metadata":{}},{"cell_type":"markdown","source":"## Problem description\n\nOne of the most important aspects of healthcare, is a timely diagnosis of a disease, as it's easier to treat in early stages. This is not only appliable on cancer, but also in infections, along with other diseases.\n\nPneumonia is swelling or inflammation of the tissue in one or both lungs. It's usually caused by a bacterial infection, but it can also be caused by a virus. The symptoms of pneumonia can develop suddenly over 24 to 48 hours, or they may come on more slowly over several days. Common symptoms of pneumonia include cough, difficulty breathing, rapid heartbeat, high temperature, sweating, shivering and chest pain, among others. This disease can lead to death, so a prompt diagnosis can increase the chance of fast recovering and survival of the individual.\n\n## Objective\n\nPneumonia is not easly identified in chest X-ray images, so the objective of this project is to create a model, that could be able to identify from images if a patient has pneumonia or not.\n\n## Data\n\n### Description\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n\n### Dataset\n\nThe dataset was obtained from the Kaggle dataset [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia), which comes from [Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification](https://data.mendeley.com/datasets/rscbjbr9sj/2) from Mendeley Data. According to de description of this dataset, data is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia or Normal). There are 5863 X-Ray images (JPEG) and 2 categories according to de diagnosis (Pneumonia or Normal).","metadata":{}},{"cell_type":"markdown","source":"## Set up","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport os\nimport re\nimport warnings\nimport eli5\nimport shap\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom math import ceil\nfrom matplotlib import gridspec\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, plot_roc_curve, plot_confusion_matrix\nfrom IPython.display import Image, display\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\ntrain_path = '../input/chest-xray-pneumonia/chest_xray/train'\ntest_path = '../input/chest-xray-pneumonia/chest_xray/test'\nvalid_path = '../input/chest-xray-pneumonia/chest_xray/val'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nEDA for images is different from what we can do with structured, text or time series data, so I took into account some [recomendations](https://towardsdatascience.com/exploratory-data-analysis-ideas-for-image-classification-d3fc6bbfb2d2) in this matter. The objective of this EDA is to identify differences between images.","metadata":{}},{"cell_type":"markdown","source":"### Image Visualization\n\nFirstly, we are going to visualize three random images from each class in order to assess any difference, with a non-medical eye clearly.","metadata":{}},{"cell_type":"code","source":"normal_imgs = [x for x in os.listdir(f'{train_path}/NORMAL') if x.endswith('.jpeg')]\npneumo_imgs = [y for y in os.listdir(f'{train_path}/PNEUMONIA') if y.endswith('.jpeg')]\n\nselect_norm = np.random.choice(normal_imgs, 3, replace = False)\nselect_pneu = np.random.choice(pneumo_imgs, 3, replace = False)\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in range(6):\n    if i < 3:\n        im_path = f'{train_path}/NORMAL/{select_norm[i]}'\n        label = 'NORMAL'\n    else:\n        im_path = f'{train_path}/PNEUMONIA/{select_pneu[i-3]}'\n        label = 'PNEUMONIA'\n    \n    ax = fig.add_subplot(2, 3, i+1)\n    \n    im = tf.keras.preprocessing.image.load_img(im_path, target_size = (100,100), color_mode='grayscale')\n    plt.imshow(im, cmap='Greys_r')\n    plt.title(label)\n    plt.axis('off')\n\nplt.show()\n\nprint(f'Number of normal images: {len(normal_imgs)}')\nprint(f'Number of pneumonia images: {len(pneumo_imgs)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In a first glance, we can se that in Pneumonia images, there is more white spots and noise in the lung area. Also, we have more Pneumonia images than Normal images, so we have a class imbalance that we need to solve.\n\nFor the next steps in the analysis, we are going to convert the images in a Numpy Array.","metadata":{}},{"cell_type":"code","source":"size = (64, 64)\n\ndef imgs2np(path, file_list, size = (64, 64)):\n    \n    for i in file_list:\n        image_path = path + i\n        image = tf.keras.preprocessing.image.load_img(image_path, target_size = size, color_mode='grayscale')\n        image = tf.keras.preprocessing.image.img_to_array(image)\n        img_ts = [image.ravel()]\n        \n        try:\n            np_array = np.concatenate((np_array, img_ts))\n\n        except NameError:\n            np_array = img_ts    \n\n    return np_array\n\nnp_normal_images = imgs2np(f'{train_path}/NORMAL/', normal_imgs)\nnp_pneumonia_images = imgs2np(f'{train_path}/PNEUMONIA/', pneumo_imgs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we are going to obtain the mean image and the standard deviation image of both classes.","metadata":{}},{"cell_type":"code","source":"mean_img_norm = np.mean(np_normal_images, axis=0).reshape(size)\nmean_img_pneum = np.mean(np_pneumonia_images, axis=0).reshape(size)\nstd_img_norm = np.std(np_normal_images, axis=0).reshape(size)\nstd_img_pneum = np.std(np_pneumonia_images, axis=0).reshape(size)\n\nax = plt.subplot(2, 2, 1)\nplt.imshow(mean_img_norm, cmap='Greys_r')\nplt.title('Average Normal', fontsize=10)\nplt.axis('off')\n\nax = plt.subplot(2, 2, 2)\nplt.imshow(std_img_norm, cmap='Greys_r')\nplt.title('Standard Deviation Normal', fontsize=10)\nplt.axis('off')\n\nax = plt.subplot(2, 2, 3)\nplt.imshow(mean_img_pneum, cmap='Greys_r')\nplt.title('Average Pneumonia', fontsize=10)\nplt.axis('off')\n\nax = plt.subplot(2, 2, 4)\nplt.imshow(std_img_pneum, cmap='Greys_r')\nplt.title('Standard Deviation Pneumonia', fontsize=10)\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the average images, that in patients without pneumonia the lungs are more clearly defined that those with pneumonia. There is also a higher standard deviation between pneumonia images, making clear the lung damage caused by the disease.","metadata":{}},{"cell_type":"markdown","source":"### Contrast Between Images\n\nNow, I am going to compare the contrast between mean images, to get information about their differences.","metadata":{}},{"cell_type":"code","source":"contrast_mean = mean_img_norm - mean_img_pneum\nplt.imshow(contrast_mean, cmap='viridis')\nplt.title(f'Difference Between Normal and Pneumonia Average')\nplt.axis('off')\nplt.colorbar()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the image we can se that there is a negative difference in the lung area, showing there is a higher contrast in this area for pneumonia images.","metadata":{}},{"cell_type":"markdown","source":"### Eigenimages\n\nFinally, I am going to analyse the eigenimages from the dataset, with a number of components enough to explain the 70% of the variance in the images.","metadata":{}},{"cell_type":"code","source":"def eigenimages(im, n_comp = 0.7, size = (64, 64)):\n    pca = PCA(n_components = n_comp, whiten = True)\n    pca.fit(im)\n    print('Number of PC: ', pca.n_components_)\n    return pca\n  \ndef plot_pca(pca, title, size = (64, 64)):\n    n_comp = pca.n_components_\n    fig = plt.figure(figsize=(8, 8))\n    rows = int(n_comp**0.5)\n    cols = ceil(n_comp/rows)\n    for i in range(n_comp):\n        ax = fig.add_subplot(rows, cols, i + 1, xticks = [], yticks = [])\n        ax.imshow(pca.components_[i].reshape(size), \n                  cmap='Greys_r')\n    plt.axis('off')\n    fig.suptitle(title)\n    plt.show()\n    \nplot_pca(eigenimages(np_normal_images), 'NORMAL')\nplot_pca(eigenimages(np_pneumonia_images), 'PNEUMONIA')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the eigenimages is clear that in Normal patients, the lung area has a well defined contour and rib cage, and in Pneumonia images this area is more diffuse.","metadata":{}},{"cell_type":"markdown","source":"## Dataset preparation\n\nNow that we have information about how differentiable are images between them, we can say that a CNN model will have no issues classifying them. Taking this into account, we can prepare the datasets to train and validate the model.","metadata":{}},{"cell_type":"code","source":"ds_train = image_dataset_from_directory(\n    train_path,\n    labels='inferred',\n    label_mode='binary',\n    image_size=(224,224),\n    batch_size=32,\n    shuffle=True,\n)\n\nds_test = image_dataset_from_directory(\n    test_path,\n    labels='inferred',\n    label_mode='binary',\n    image_size=(224,224),\n    batch_size=32,\n    shuffle=False,\n)\n\nds_valid = image_dataset_from_directory(\n    valid_path,\n    labels='inferred',\n    label_mode='binary',\n    image_size=(224,224),\n    batch_size=1,\n    shuffle=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am going to convert the image data to float to get better performance from the model.","metadata":{}},{"cell_type":"code","source":"def convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nds_train = ds_train.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\nds_test = ds_test.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\nds_valid = ds_valid.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\nOne possible approach to this problem is convert the images to structured data, extrating relevant information like contrast, brightness, Fourier transform, among others, but an approach with CNN can help us extract relevant features related to pixel positions, which is what we are seeking when we want to get data from specific areas in the image, and the relationships between pixels in it. Taking this into account, I am going to use TensorFlow to build the classification model.","metadata":{}},{"cell_type":"markdown","source":"### Training hyperparameters\n\nAs we have a binary classification model, I can define some binary metrics like those from the confussion matrix, and the metrics that derive from it (AUC, Precission, Recall and PR ratio).","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.BinaryCrossentropy()\nmetrics = [tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n           tf.keras.metrics.TruePositives(name='TP'),\n           tf.keras.metrics.FalsePositives(name='FP'),\n           tf.keras.metrics.TrueNegatives(name='TN'),\n           tf.keras.metrics.FalseNegatives(name='FN'),\n           tf.keras.metrics.Precision(name='P'),\n           tf.keras.metrics.Recall(name='R'),\n           tf.keras.metrics.AUC(name='AUC'),\n           tf.keras.metrics.AUC(name='PR', curve='PR')\n          ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a CNN\n\nCurrently, we have available a lot of pretrained models to extract better insights from image data. But first, the best is to build a CNN to check if a simpler model is enough for this problem. Also, we are going to add some regularization to the convolutional layers, to prevent any overfitting.","metadata":{}},{"cell_type":"code","source":"regularizer = tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n\nmodel_c = tf.keras.models.Sequential([\n    tf.keras.layers.Rescaling(1./255, input_shape=(224, 224, 3)),\n    tf.keras.layers.RandomFlip('horizontal'),\n    tf.keras.layers.Conv2D(8, 3, padding='same', activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizer),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel_c.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_c.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nhistory_c = model_c.fit(ds_train,\n                        validation_data=ds_test,\n                        epochs=50,\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame_c = pd.DataFrame(history_c.history)\nhistory_frame_c.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame_c.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transfer learning - VGG16","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224,224,3))\nnet = tf.keras.layers.Rescaling(1./255)(inputs)\nnet = tf.keras.layers.RandomFlip('horizontal')(net)\n\npretrained_base = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                    pooling='avg',\n                                                    input_tensor=net\n                                                   )\npretrained_base.trainable = False\n\nnet = tf.keras.layers.Dropout(0.2)(pretrained_base.output)\nnet = tf.keras.layers.Dense(16, activation='relu')(net)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n\nmodel_t = tf.keras.Model(inputs, outputs)\n\nmodel_t.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_t = model_t.fit(ds_train,\n                        validation_data=ds_test,\n                        epochs=50,\n                       )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame_t = pd.DataFrame(history_t.history)\nhistory_frame_t.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame_t.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t.evaluate(ds_valid, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training VGG 16","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224,224,3))\nnet = tf.keras.layers.Rescaling(1./255)(inputs)\nnet = tf.keras.layers.RandomFlip('horizontal')(net)\n\npretrained_base = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                    pooling='avg',\n                                                    input_tensor=net\n                                                   )\npretrained_base.trainable = True\n\nnet = tf.keras.layers.Dropout(0.2)(pretrained_base.output)\nnet = tf.keras.layers.Dense(16, activation='relu')(net)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n\nmodel_t_2 = tf.keras.Model(inputs, outputs)\n\nmodel_t_2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_2.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_t_2 = model_t_2.fit(ds_train,\n                            validation_data=ds_test,\n                            epochs=50,\n                           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame_t_2 = pd.DataFrame(history_t_2.history)\nhistory_frame_t_2.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame_t_2.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_2.evaluate(ds_valid, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transfer learning - VGG16 Max pool","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(224,224,3))\nnet = tf.keras.layers.Rescaling(1./255)(inputs)\nnet = tf.keras.layers.RandomFlip('horizontal')(net)\n\npretrained_base = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                    pooling='max',\n                                                    input_tensor=net\n                                                   )\npretrained_base.trainable = False\n\nnet = tf.keras.layers.Dropout(0.2)(pretrained_base.output)\nnet = tf.keras.layers.Dense(16, activation='relu')(net)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n\nmodel_t_3 = tf.keras.Model(inputs, outputs)\n\nmodel_t_3.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_3.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_t_3 = model_t_3.fit(ds_train,\n                            validation_data=ds_test,\n                            epochs=50,\n                           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame_t_3 = pd.DataFrame(history_t_3.history)\nhistory_frame_t_3.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame_t_3.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_3.evaluate(ds_valid, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transfer learing VGG16 - Avg Pool, Regularization","metadata":{}},{"cell_type":"code","source":"regularizer = tf.keras.regularizers.L1L2(l2=0.01)\n\ninputs = tf.keras.Input(shape=(224,224,3))\nnet = tf.keras.layers.Rescaling(1./255)(inputs)\nnet = tf.keras.layers.RandomFlip('horizontal')(net)\n\npretrained_base = tf.keras.applications.vgg16.VGG16(include_top=False,\n                                                    pooling='avg',\n                                                    input_tensor=net\n                                                   )\npretrained_base.trainable = False\n\nnet = tf.keras.layers.Dropout(0.2)(pretrained_base.output)\nnet = tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizer)(net)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n\nmodel_t_4 = tf.keras.Model(inputs, outputs)\n\nmodel_t_4.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_4.compile(optimizer=optimizer, loss=loss, metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_t_4 = model_t_4.fit(ds_train,\n                            validation_data=ds_test,\n                            epochs=50,\n                           )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_frame_t_4 = pd.DataFrame(history_t_4.history)\nhistory_frame_t_4.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame_t_4.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_t_4.evaluate(ds_valid, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization and Explainability","metadata":{}},{"cell_type":"code","source":"def get_img_array(img_path, size):\n    \n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    array = tf.keras.preprocessing.image.img_to_array(img)\n    array = np.expand_dims(array, axis=0)\n    return array\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        model.inputs,\n        [model.get_layer(last_conv_layer_name).output,\n         model.output\n        ]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        p = grad_model(img_array)\n        last_conv_layer_output = p[0]\n        preds = p[1]\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n    return heatmap.numpy()\n\ndef display_gradcam(img_path, heatmap, size, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n    img = tf.keras.preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n    \n    # Display Grad-CAM\n    \n    plt.imshow(superimposed_img)\n    plt.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_imgs_val = [x for x in os.listdir(f'{valid_path}/NORMAL') if x.endswith('.jpeg')]\npneumo_imgs_val = [y for y in os.listdir(f'{valid_path}/PNEUMONIA') if y.endswith('.jpeg')]\n\nselect_norm_val = np.random.choice(normal_imgs_val, 1, replace = False)\nselect_pneu_val = np.random.choice(pneumo_imgs_val, 1, replace = False)\n\npath = f'{valid_path}/NORMAL/{select_norm_val[0]}'\n\nimg_array = get_img_array(path, size=(224, 224, 3))\n\nheatmap = make_gradcam_heatmap(img_array, model_t_4, 'block5_conv3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_gradcam(path, heatmap, size=(224,224))\nplt.title('Normal')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = f'{valid_path}/PNEUMONIA/{select_pneu_val[0]}'\n\nimg_array = get_img_array(path, size=(224, 224, 3))\n\nheatmap = make_gradcam_heatmap(img_array, model_t_4, 'block5_conv3')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_gradcam(path, heatmap, size=(224,224))\nplt.title('Pneumonia')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References","metadata":{}}]}